load(url("https://cbail.github.io/trumptweets.Rdata"))
trumptweets<-read.csv(url("https://cbail.github.io/trumptweets.csv"))
trumptweets<-read.csv(url("https://cbail.github.io/trumptweets.csv"),
stringsAsFactors = FALSE)
trumptweets<-trumptweets[29000:32826,]
trumptweets$text<-gsub("http.*|@*|t.co|amp|RT","", trumptweets$text)
text <- trumptweets$text %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
library(dplyr)
trumptweets$text<-gsub("http.*|@*|t.co|amp|RT","", trumptweets$text)
text <- trumptweets$text %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
library(dplyr)
library(stringr)
trumptweets$text<-gsub("http.*|@*|t.co|amp|RT","", trumptweets$text)
text <- trumptweets$text %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("corpus length: %d", length(text)))
chars <- text %>%
unique() %>%
sort()
print(sprintf("total chars: %d", length(chars)))
chars <- text %>%
unique() %>%
sort()
text <- trumptweets$text %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
trumptweets$text<-gsub("http.*|@*|t.co|amp|RT","", trumptweets$text)
text <- trumptweets$text %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
library(tokenizers)
library(dplyr)
library(stringr)
library(tokenizers)
trumptweets$text<-gsub("http.*|@*|t.co|amp|RT","", trumptweets$text)
text <- trumptweets$text %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
chars <- text %>%
unique() %>%
sort()
print(sprintf("total chars: %d", length(chars)))
print(sprintf("corpus length: %d", length(text)))
print(sprintf("corpus length: %d", length(text)))
maxlen <- 140
dataset <- map(
seq(1, length(text) - maxlen - 1, by = 3),
~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
library(purrr)
dataset <- map(
seq(1, length(text) - maxlen - 1, by = 3),
~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
dataset <- transpose(dataset)
x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))
for(i in 1:length(dataset$sentece)){
x[i,,] <- sapply(chars, function(x){
as.integer(x == dataset$sentece[[i]])
})
y[i,] <- as.integer(chars == dataset$next_char[[i]])
}
library(keras)
library(readr)
library(stringr)
library(purrr)
library(tokenizers)
#install_keras()
maxlen <- 140
# Data Preparation --------------------------------------------------------
# # Retrieve text
# path <- get_file(
#   'nietzsche.txt',
#   origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'
# )
trumptweets<-read.csv("~/Desktop/trumptweets.csv")
trumptweets<-trumptweets[29000:32826,]
trumptweets$text<-as.character(trumptweets$text)
#drop hyperlinks and other Twitter junk
trumptweets$text<-gsub("http.*|@*|t.co|amp|RT","", trumptweets$text)
# Load, collapse, and tokenize text
text <- trumptweets$text %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("corpus length: %d", length(text)))
chars <- text %>%
unique() %>%
sort()
print(sprintf("total chars: %d", length(chars)))
# Cut the text in semi-redundant sequences of maxlen characters
dataset <- map(
seq(1, length(text) - maxlen - 1, by = 3),
~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
dataset <- transpose(dataset)
# Vectorization
x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))
for(i in 1:length(dataset$sentece)){
x[i,,] <- sapply(chars, function(x){
as.integer(x == dataset$sentece[[i]])
})
y[i,] <- as.integer(chars == dataset$next_char[[i]])
}
# Model Definition --------------------------------------------------------
model <- keras_model_sequential()
model %>%
layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%
layer_dense(length(chars)) %>%
layer_activation("softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
# Training & Results ----------------------------------------------------
sample_mod <- function(preds, temperature = 1){
preds <- log(preds)/temperature
exp_preds <- exp(preds)
preds <- exp_preds/sum(exp(preds))
rmultinom(1, 1, preds) %>%
as.integer() %>%
which.max()
}
on_epoch_end <- function(epoch, logs) {
cat(sprintf("epoch: %02d ---------------\n\n", epoch))
for(diversity in c(0.2, 0.5, 1, 1.2)){
cat(sprintf("diversity: %f ---------------\n\n", diversity))
start_index <- sample(1:(length(text) - maxlen), size = 1)
sentence <- text[start_index:(start_index + maxlen - 1)]
generated <- ""
for(i in 1:400){
x <- sapply(chars, function(x){
as.integer(x == sentence)
})
x <- array_reshape(x, c(1, dim(x)))
preds <- predict(model, x)
next_index <- sample_mod(preds, diversity)
next_char <- chars[next_index]
generated <- str_c(generated, next_char, collapse = "")
sentence <- c(sentence[-1], next_char)
}
cat(generated)
cat("\n\n")
}
}
print_callback <- callback_lambda(on_epoch_end = on_epoch_end)
out<-model %>% fit(
x, y,
batch_size = 400,
epochs = 30,
callbacks = print_callback
)
print_callback <- callback_lambda(on_epoch_end = on_epoch_end)
print_callback
out<-model %>% fit(
x, y,
batch_size = 400,
epochs = 25,
callbacks = print_callback
)
library(keras)
library(readr)
library(stringr)
library(purrr)
library(tokenizers)
#install_keras()
maxlen <- 140
# Data Preparation --------------------------------------------------------
# # Retrieve text
# path <- get_file(
#   'nietzsche.txt',
#   origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'
# )
trumptweets<-read.csv("~/Desktop/trumptweets.csv")
trumptweets<-trumptweets[29500:32826,]
trumptweets$text<-as.character(trumptweets$text)
#drop hyperlinks and other Twitter junk
trumptweets$text<-gsub("http.*|@*|t.co|amp|RT","", trumptweets$text)
# Load, collapse, and tokenize text
text <- trumptweets$text %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("corpus length: %d", length(text)))
chars <- text %>%
unique() %>%
sort()
print(sprintf("total chars: %d", length(chars)))
# Cut the text in semi-redundant sequences of maxlen characters
dataset <- map(
seq(1, length(text) - maxlen - 1, by = 3),
~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
dataset <- transpose(dataset)
# Vectorization
x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))
for(i in 1:length(dataset$sentece)){
x[i,,] <- sapply(chars, function(x){
as.integer(x == dataset$sentece[[i]])
})
y[i,] <- as.integer(chars == dataset$next_char[[i]])
}
# Model Definition --------------------------------------------------------
model <- keras_model_sequential()
model %>%
layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%
layer_dense(length(chars)) %>%
layer_activation("softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
# Training & Results ----------------------------------------------------
sample_mod <- function(preds, temperature = 1){
preds <- log(preds)/temperature
exp_preds <- exp(preds)
preds <- exp_preds/sum(exp(preds))
rmultinom(1, 1, preds) %>%
as.integer() %>%
which.max()
}
on_epoch_end <- function(epoch, logs) {
cat(sprintf("epoch: %02d ---------------\n\n", epoch))
for(diversity in c(0.2, 0.5, 1, 1.2)){
cat(sprintf("diversity: %f ---------------\n\n", diversity))
start_index <- sample(1:(length(text) - maxlen), size = 1)
sentence <- text[start_index:(start_index + maxlen - 1)]
generated <- ""
for(i in 1:400){
x <- sapply(chars, function(x){
as.integer(x == sentence)
})
x <- array_reshape(x, c(1, dim(x)))
preds <- predict(model, x)
next_index <- sample_mod(preds, diversity)
next_char <- chars[next_index]
generated <- str_c(generated, next_char, collapse = "")
sentence <- c(sentence[-1], next_char)
}
cat(generated)
cat("\n\n")
}
}
print_callback <- callback_lambda(on_epoch_end = on_epoch_end)
out<-model %>% fit(
x, y,
batch_size = 400,
epochs = 25,
callbacks = print_callback
)
